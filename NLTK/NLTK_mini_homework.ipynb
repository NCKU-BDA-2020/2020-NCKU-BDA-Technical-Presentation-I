{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing Using NLTK\n",
    "The training data contains about 8,000 comments with corresponding stars(1 to 5). Assume that we're going to train a model to predict the star by the comment. In this homework, we're going to implement data preprocessing by using NLTK. The steps are shown below.\n",
    "<br>\n",
    "<li> Import the packages you need and read the csv file.\n",
    "<li> Turn each comment into a word bag. Remember the bag only contain verb and adjective, stop words and punctuations are excluded.\n",
    "<li> Turn the word bag into number using one-hot encoding. Each row represents the sample, and each column represent the word.\n",
    "<li> Finally, using the train_test_split function in sklearn.model_selection to split the data into training set and testing set. Then put the training set into the model.(This part of code is provided.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review_id</th>\n",
       "      <th>business_id</th>\n",
       "      <th>user_id</th>\n",
       "      <th>text</th>\n",
       "      <th>date</th>\n",
       "      <th>stars</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3223</td>\n",
       "      <td>2055</td>\n",
       "      <td>2533</td>\n",
       "      <td>Sometimes things happen, and when they do this...</td>\n",
       "      <td>2010/12/30</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9938</td>\n",
       "      <td>4165</td>\n",
       "      <td>6371</td>\n",
       "      <td>I know Kerrie through my networking and we ben...</td>\n",
       "      <td>2011/4/26</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7123</td>\n",
       "      <td>869</td>\n",
       "      <td>4929</td>\n",
       "      <td>Love their pizza!!!\\nVery fresh. Their cannoli...</td>\n",
       "      <td>2012/9/28</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3601</td>\n",
       "      <td>1603</td>\n",
       "      <td>2789</td>\n",
       "      <td>Being from NJ I am always on the prowl for my ...</td>\n",
       "      <td>2009/6/7</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3948</td>\n",
       "      <td>2347</td>\n",
       "      <td>1245</td>\n",
       "      <td>We have tried this spot a few times and each v...</td>\n",
       "      <td>2011/2/20</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   review_id  business_id  user_id  \\\n",
       "0       3223         2055     2533   \n",
       "1       9938         4165     6371   \n",
       "2       7123          869     4929   \n",
       "3       3601         1603     2789   \n",
       "4       3948         2347     1245   \n",
       "\n",
       "                                                text        date  stars  \n",
       "0  Sometimes things happen, and when they do this...  2010/12/30      5  \n",
       "1  I know Kerrie through my networking and we ben...   2011/4/26      5  \n",
       "2  Love their pizza!!!\\nVery fresh. Their cannoli...   2012/9/28      5  \n",
       "3  Being from NJ I am always on the prowl for my ...    2009/6/7      4  \n",
       "4  We have tried this spot a few times and each v...   2011/2/20      4  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.sparse import csr_matrix\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "from nltk.corpus import wordnet\n",
    "import string\n",
    "\n",
    "df = pd.read_csv(\"training_data.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<li>Write a function to turn all the comments into wordbag, and pick up verbs and adjectives only.\n",
    "<br>1.input the \"text\" column in df (i.e. df.text), and tokenize all the comments(nltk.word_tokenize() )\n",
    "<br>2. pick up all the stop words and punctuation (string.punctuation and nltk.corpus.stopwords.words('english')  )\n",
    "<br>3. pos_tag the remain words, and pick up lemmatized verbs and  lemmatized adjectives only.(nltk.pos_tag()  and wnl.lemmatize())\n",
    "<br>4. return a list which contains dictionaries, each dictionary is a comment, i.e.\n",
    "<br>[{'happen': 1, 'want': 1, 'take': 1, 'best': 1, 'nice': 1, 'find': 1},\n",
    " {'know': 1,\n",
    "  'kerrie': 2,\n",
    "  'benefit': 1,\n",
    "  'need': 3,\n",
    "  'plan': 1,\n",
    "  'remind': 1,\n",
    "1},\n",
    " {'love': 1, 'fresh': 1, 'good': 1, 'seem': 1, 'great': 1},\n",
    " {'hometown': 1,\n",
    "  'italian': 1,\n",
    "  'best': 1,\n",
    "  'pizza': 1,\n",
    "  'big': 1,}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'happen': 1, 'want': 1, 'take': 1, 'best': 1, 'nice': 1, 'find': 1},\n",
       " {'know': 1,\n",
       "  'kerrie': 2,\n",
       "  'benefit': 1,\n",
       "  'need': 3,\n",
       "  'plan': 1,\n",
       "  'remind': 1,\n",
       "  'true': 1,\n",
       "  'come': 1,\n",
       "  'troubled': 1,\n",
       "  'good': 1,\n",
       "  'look': 1,\n",
       "  'help': 1},\n",
       " {'love': 1, 'fresh': 1, 'good': 1, 'seem': 1, 'great': 1},\n",
       " {'hometown': 1,\n",
       "  'italian': 1,\n",
       "  'best': 1,\n",
       "  'pizza': 1,\n",
       "  'big': 1,\n",
       "  'many': 1,\n",
       "  'awesome': 1,\n",
       "  'prefer': 1,\n",
       "  'unsweetened': 1,\n",
       "  'great': 2,\n",
       "  'new': 1,\n",
       "  'grab': 1,\n",
       "  'special': 1},\n",
       " {'try': 2,\n",
       "  'find': 1,\n",
       "  'like': 1,\n",
       "  'keep': 1,\n",
       "  'come': 1,\n",
       "  'favorite': 1,\n",
       "  'extensive': 1,\n",
       "  'oversized': 1,\n",
       "  'wonderful': 1,\n",
       "  'add': 1},\n",
       " {'good': 2,\n",
       "  'seem': 1,\n",
       "  'knowledgeable': 1,\n",
       "  'put': 1,\n",
       "  'get': 1,\n",
       "  'need': 1,\n",
       "  'light': 1,\n",
       "  'say': 1,\n",
       "  'sell': 1,\n",
       "  'miss': 1,\n",
       "  'proceed': 1,\n",
       "  'rip': 1},\n",
       " {'go': 2,\n",
       "  'give': 1,\n",
       "  'small': 1,\n",
       "  'seem': 1,\n",
       "  'little': 2,\n",
       "  'fresh': 2,\n",
       "  'taste': 1,\n",
       "  'salsa': 1,\n",
       "  'killer': 1,\n",
       "  'green': 1,\n",
       "  'carne': 3,\n",
       "  'cheese': 1,\n",
       "  'standard': 1,\n",
       "  'real': 1,\n",
       "  'metric': 1,\n",
       "  'pile': 1,\n",
       "  'fry': 1,\n",
       "  'french': 1,\n",
       "  'sour': 1,\n",
       "  'beat': 1,\n",
       "  'hammer': 1,\n",
       "  'classy': 1,\n",
       "  'wary': 1,\n",
       "  'stupid': 1,\n",
       "  'best': 1,\n",
       "  'like': 1},\n",
       " {'go': 2,\n",
       "  'lie': 1,\n",
       "  'describe': 1,\n",
       "  'feel': 1,\n",
       "  'mention': 1,\n",
       "  'much': 1,\n",
       "  'look': 1,\n",
       "  'deliver': 1,\n",
       "  'favorite': 1,\n",
       "  'guilty': 1,\n",
       "  'encourage': 1,\n",
       "  'try': 2,\n",
       "  'first': 1,\n",
       "  'take': 1,\n",
       "  'allow': 1,\n",
       "  'guide': 1,\n",
       "  'thru': 1,\n",
       "  'offer': 1,\n",
       "  'complimentary': 1,\n",
       "  'popular': 1,\n",
       "  'learn': 1,\n",
       "  'moderately': 1,\n",
       "  'challenging': 1,\n",
       "  'get': 1,\n",
       "  'super': 1,\n",
       "  'double': 1,\n",
       "  'red': 1,\n",
       "  'refried': 1,\n",
       "  'fried': 1,\n",
       "  'bake': 1,\n",
       "  'love': 1},\n",
       " {'roast': 1,\n",
       "  'garlic': 1,\n",
       "  'excellent': 1,\n",
       "  'sprinkle': 1,\n",
       "  'tortilla': 1,\n",
       "  'tiny': 1,\n",
       "  'sour': 1,\n",
       "  'mixed': 1,\n",
       "  'green': 1,\n",
       "  'delightful': 1,\n",
       "  'turkey': 1,\n",
       "  'chicken': 1,\n",
       "  'pesto': 1,\n",
       "  'drip': 1,\n",
       "  'espresso': 1,\n",
       "  'good': 1,\n",
       "  'overall': 1,\n",
       "  'great': 1,\n",
       "  'make': 1,\n",
       "  'fresh': 1,\n",
       "  'seat': 1,\n",
       "  'cozy': 1,\n",
       "  'little': 2,\n",
       "  'set': 1,\n",
       "  'look': 2,\n",
       "  'accommodate': 1,\n",
       "  'swivel': 1,\n",
       "  'perfect': 1,\n",
       "  'savor': 1,\n",
       "  'din': 1,\n",
       "  'fill': 1,\n",
       "  'declare': 1,\n",
       "  'maximum': 1,\n",
       "  'peak': 1,\n",
       "  'avoid': 1,\n",
       "  'appear': 1,\n",
       "  'feenin': 1,\n",
       "  'special': 1,\n",
       "  'forget': 1,\n",
       "  'get': 1,\n",
       "  'work': 1,\n",
       "  'factor': 1,\n",
       "  'impossible': 1,\n",
       "  'networking': 1,\n",
       "  'receive': 1,\n",
       "  'locate': 1,\n",
       "  'free': 2,\n",
       "  'include': 1,\n",
       "  'tasty': 1,\n",
       "  'open': 1,\n",
       "  'sun': 1},\n",
       " {'fancy': 1,\n",
       "  'sprinkle': 1,\n",
       "  'nm': 1,\n",
       "  'cafe': 1,\n",
       "  'make': 1,\n",
       "  'get': 1,\n",
       "  'best': 1,\n",
       "  'work': 1}]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wnl = WordNetLemmatizer()\n",
    "\n",
    "def tokenize_document(lis_text):\n",
    "    outp = list()\n",
    "    \"\"\"\n",
    "    write sth. here\n",
    "    \"\"\"\n",
    "    return outp\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<li>Write a function to turn the bag of word into numeric numpy array by one-hot encoding method.\n",
    "<br>1. Input the list from the return of above function.\n",
    "<br>2. create a python set, called \"features\", containing all the word in all comments.  ex:{\"I\", \"have\", \"a\", \"dog\", \"cat\"}\n",
    "<br>3. create a nested list, called mat, containg the counts of word in each comment.  \n",
    "<br>ex:  [{\"I\":1, \"have\":1, \"a\":1, \"dog\":1}, {\"I\":1, \"have\":1, \"a\":1, \"cat\":1}] -->[[1,1,1,1,0], [1,1,1,0,1]]\n",
    "<br>4. put the nested into numpy.array() and return the array and \"features\" set as the function results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       ...,\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def vectorize_mat(token_lis):\n",
    "    features = set()\n",
    "    \"\"\"\n",
    "    write sth. here\n",
    "    \"\"\"\n",
    "    mat = list()\n",
    "    \"\"\"\n",
    "    write sth. here\n",
    "    \"\"\"\n",
    "    return features,np.array(mat)\n",
    "features,one_hot_mat = vectorize_mat(tokenize_document(df.text))\n",
    "one_hot_mat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we choose Multinomial Naive Bayes Classifier as the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7997, 10189)\n",
      "Under MNB model, MSE and accuracy are 1.036 and 0.475, respectively.\n"
     ]
    }
   ],
   "source": [
    "train_data, test_data, train_lab, test_lab = train_test_split(csr_matrix(one_hot_mat), df.stars\n",
    "                                                              , train_size = 0.8, random_state = 123)\n",
    "MNB_model = MultinomialNB(alpha = 0.5)\n",
    "MNB_model.fit(train_data, train_lab)\n",
    "MSE = np.std(MNB_model.predict(test_data) - test_lab)\n",
    "ACC = MNB_model.score(test_data, test_lab)\n",
    "print(one_hot_e1.shape)\n",
    "print(\"Under MNB model, MSE and accuracy are %.3f and %.3f, respectively.\" % (MSE, ACC) )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
